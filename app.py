# ---------------------------------
# Sidebar Navigation (Commented out, using tabs instead)
# ---------------------------------
# st.sidebar.title("App Journey")
# page = st.sidebar.radio("Go To", ["ğŸ  Home", "ğŸ§¹ Data Cleaning", "ğŸ“Š EDA", "ğŸ¤– Modeling", "ğŸš€ Deployment"])

import streamlit as st
from pipeline.data import load_data, clean_data
from pipeline.eda import perform_eda
from pipeline.modeling import identify_problem, build_clustering_models
import subprocess
import time
import os
import io
import pickle
import sys
import requests  # Added for API calls in inference UI

# ---------------------------------
# App Configuration
# ---------------------------------
st.set_page_config(
    page_title='ML Forge | Auto-ML',
    page_icon='ğŸ¤–',
    layout='wide',
    initial_sidebar_state='auto'
)

# ---------------------------------
# Main Page Content
# ---------------------------------
st.title("ML Forge App")
st.markdown("**Smoothly, Build your Model**")
st.info("This platform automates the ML lifecycle: upload data, clean, explore, model, and deploy. Follow the tabs step-by-step.")

# ---------------------------------
# Session State Init -> Store uploaded data across pages
# ---------------------------------
if "df" not in st.session_state:
    st.session_state.df = None
    st.session_state.cleaned = False
    st.session_state.cleaned_df = None

if "chat_open" not in st.session_state:
    st.session_state.chat_open = False

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "api_process" not in st.session_state:
    st.session_state.api_process = None  # To store the API subprocess

# ---------------------------------
# Tabs
# ---------------------------------
tab1, tab2, tab3, tab4, tab5 = st.tabs(
    ["ğŸ  Home", "ğŸ§¹ Data Cleaning", "ğŸ“Š EDA", "ğŸ¤– Modeling", "ğŸš€ Deployment"]
)

# ---------------------------------
# Home Page (Data Upload)
# ---------------------------------
with tab1:
    st.header("ğŸ¡ Welcome to ML Life Cycle Platform")
    st.markdown("Upload your dataset here to start the pipeline.")
    uploaded_file = st.file_uploader("Upload dataset", type=['csv', 'json', 'xlsx'], help="Supported formats: CSV, JSON, Excel. Data will be loaded and previewed.")
    if uploaded_file is not None:
        try:
            df = load_data(uploaded_file)
            # Save data in Session
            st.session_state.df = df
            st.session_state.cleaned = False
            st.write("#### ğŸ§¾ Raw Data Preview")
            st.dataframe(df.head())
            st.write(f"Shape = {df.shape}")
            st.write("#### Descriptive Statistics")
            st.dataframe(df.describe(include='all'))
            st.write("#### MetaData")
            buffer = io.StringIO()
            df.info(buf=buffer)
            st.text(buffer.getvalue())
            st.write("##### Missing Values")
            st.table(df.isna().sum())
            st.write(f"##### Number of Duplicates Values = {df.duplicated().sum()}")
        except ValueError as e:
            st.error(str(e))

# ---------------------------------
# Data Cleaning Page
# ---------------------------------
with tab2:
    st.header("ğŸ§½ Data Cleaning")
    if st.session_state.df is not None:
        df_clean, clean_log, run_id = clean_data(st.session_state.df)  # returns log and run_id
        # Update the Data
        st.session_state.cleaned_df = df_clean
        st.session_state.cleaned = True
        st.success("âœ¨ Data Cleaning Completed Successfully!")
        st.write("### ğŸ“ Cleaning Summary")
        st.json(clean_log)
        st.download_button("Download Cleaned Data", df_clean.to_csv(index=False).encode('utf-8'), "cleaned_data.csv")
        st.info(f"MLflow Run ID: {run_id} - Check MLflow UI for logged parameters.")
        st.write("### ğŸ§¾ Cleaned Data Preview")
        st.dataframe(df_clean.head())
        st.write(f"Shape = {df.shape}")
    else:
        st.warning("âš ï¸ Please upload data from the Home page first.")

# ---------------------------------
# Dashboard Page
# ---------------------------------
with tab3:
    st.header("ğŸ“Š AI-Powered Exploratory Data Analysis Dashboard")
    st.markdown("Explore your data with statistics, visualizations, and AI suggestions.")
    if st.session_state.df is not None:
        perform_eda(st.session_state.cleaned_df)
    else:
        st.warning("âš ï¸ Please upload and clean data before performing EDA.")

# ---------------------------------
# Modeling Page
# ---------------------------------
# with tab4:
#     st.header("Build and Train Models")
#     st.markdown("Select problem type and target, then train multiple models automatically.")

#     if st.session_state.df is None:
#         st.warning("Please upload and clean data before modeling.")
#         st.stop()

#     # ------------------ Ø§Ø®ØªÙŠØ§Ø± Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ------------------
#     problem = st.selectbox(
#         "Problem Type",
#         ["Classification", "Regression", "Clustering"],
#         key="problem_select",
#         help="Choose the ML task: Classification (categories), Regression (numbers), Clustering (groups)."
#     )

#     # ------------------ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‡Ø¯Ù (Ù„Ù„Ù€ Supervised ÙÙ‚Ø·) ------------------
#     if problem != "Clustering":
#         target = st.selectbox(
#             "Select target column",
#             st.session_state.df.columns.tolist(),
#             key="target_select",
#             help="The column you want to predict."
#         )
#     else:
#         target = None

#     # ------------------ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª ------------------
#     available_models_dict = {
#         "Classification": ['LogisticRegression', 'RandomForestClassifier', 'XGBClassifier', 'MLPClassifier', 'KNeighborsClassifier', 'SVC', 'DecisionTreeClassifier'],
#         "Regression": ['LinearRegression', 'RandomForestRegressor', 'XGBRegressor', 'MLPRegressor', 'KNeighborsRegressor', 'SVR', 'DecisionTreeRegressor'],
#         "Clustering": ['KMeans', 'DBSCAN']
#     }

#     available_models = available_models_dict.get(problem, [])

#     if problem == "Clustering":
#         selected_models = st.selectbox("Select Algorithm", available_models, key="algo_select")
#     else:
#         selected_models = st.multiselect(
#             "Select Models to Train",
#             available_models,
#             default=available_models[:3],
#             key="models_select"
#         )

#     # ------------------ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ------------------
#     search_type = st.selectbox(
#         "Hyperparameter Search Type",
#         ["Random", "Grid", "Optuna"],
#         key="search_type",
#         help="Random & Optuna are faster. Grid is exhaustive."
#     )

#     n_trials = st.slider("Number of Trials (for Random/Optuna)", 5, 50, 15, key="n_trials")

#     confirm_modeling = st.checkbox(
#         "Confirm: I understand training may take time (especially with Optuna/Grid).",
#         key="confirm_train"
#     )

#     if confirm_modeling and st.button("Start Modeling", type="primary", width='stretch'):
#         if problem != "Clustering" and target is None:
#             st.error("Please select a target column for Classification/Regression.")
#         else:
#             with st.spinner("Training models in progress... This may take 1â€“3 minutes"):
#                 progress = st.progress(0)
#                 for i in range(100):
#                     time.sleep(0.01)
#                     progress.progress(i + 1)

#                 identify_problem(
#                     df=st.session_state.df,
#                     problem=problem,
#                     target=target,
#                     selected_models=selected_models,
#                     search_type=search_type,
#                     n_trials=n_trials
#                 )
#             st.success("Model training completed successfully!")
#             st.rerun()

#     if problem == "Clustering" and st.session_state.get("training_done"):
#         with st.spinner("Preparing data for clustering..."):
#             X_train, X_test, _, _, _, _ = prepare_df(st.session_state.df)
#             build_clustering_models(X_train, X_test, algorithm=selected_models)

#     has_supervised_models = st.session_state.get("best_models_trained") is not None
#     has_clustering_model = st.session_state.get("best_clustering_model") is not None

#     if has_supervised_models or has_clustering_model:
#         st.markdown("---")
#         st.success("Training completed successfully! Now save the best model")

#         col1, col2 = st.columns([3, 1])
#         with col1:
#             st.write("**Ready to deploy your model as an API**")
#         with col2:
#             if st.button("Save & Deploy Model", type="primary", width='stretch', key="save_deploy_btn"):
#                 st.session_state.show_save_section = True

#         # ------------------ Ù‚Ø³Ù… Ø§Ù„Ø­ÙØ¸ (ÙŠØ¸Ù‡Ø± Ø¨Ø¹Ø¯ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ø²Ø±) ------------------
#         if st.session_state.get("show_save_section", False):
#             st.markdown("### Confirm Model Saving")

#             # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø¨Ø£Ù…Ø§Ù†
#             target_encoder = st.session_state.get("target_encoder_saved")
#             feature_names_saved = st.session_state.get("feature_names_saved")
#             original_feature_names = st.session_state.get("original_feature_names", [])
#             original_dtypes = st.session_state.get("original_dtypes", {})
#             problem_type = st.session_state.get("problem_type", "").lower()

#             # ØªØ­Ø¯ÙŠØ¯ Ø£ÙØ¶Ù„ Ù…ÙˆØ¯ÙŠÙ„ Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
#             if problem_type == "clustering":
#                 best_model = st.session_state.get("best_clustering_model")
#                 if not best_model:
#                     st.error("Clustering model not found!")
#                     st.stop()
#                 best_name = best_model.__class__.__name__
#             else:
#                 models_dict = st.session_state.get("best_models_trained", {})
#                 if not models_dict:
#                     st.error("No trained models found!")
#                     st.stop()

#                 if problem_type == "regression":
#                     priority = ["XGBRegressor", "RandomForestRegressor", "LinearRegression"]
#                 else:
#                     priority = ["XGBClassifier", "RandomForestClassifier", "LogisticRegression"]

#                 best_name = next((n for n in priority if n in models_dict), list(models_dict.keys())[0])
#                 best_model = models_dict[best_name]

#             st.success(f"**Selected Model:** {best_name}")

#             # Ø­ÙØ¸ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
#             if st.button("Confirm Save Model Now", type="primary", width='stretch'):
#                 import os
#                 os.makedirs("models", exist_ok=True)

#                 with st.spinner("Saving model and metadata..."):
#                     # Ø­ÙØ¸ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
#                     with open("models/best_model.pkl", "wb") as f:
#                         pickle.dump(best_model, f)

#                     # Ø­ÙØ¸ Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª
#                     with open("models/feature_names.pkl", "wb") as f:
#                         pickle.dump(feature_names_saved, f)
#                     with open("models/original_feature_names.pkl", "wb") as f:
#                         pickle.dump(original_feature_names, f)
#                     with open("models/original_dtypes.pkl", "wb") as f:
#                         pickle.dump(original_dtypes, f)
#                     if target_encoder is not None:
#                         with open("models/target_encoder.pkl", "wb") as f:
#                             pickle.dump(target_encoder, f)

#                 st.success("All files saved successfully!")
#                 st.balloons()
#                 st.info("Go to the **Deployment** tab â†’ Click **Run API**")
#                 st.markdown("""
#                 ### Files Saved:
#                 - `best_model.pkl`
#                 - `feature_names.pkl`
#                 - `original_feature_names.pkl`
#                 - `original_dtypes.pkl`
#                 - `target_encoder.pkl` (if applicable)
#                 """)
# =============================================

with tab4:
    st.header("Build and Train Models")
    st.markdown(
        "Select problem type and target, then train multiple models automatically.")

    # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª + ØªÙ†Ø¸ÙŠÙÙ‡Ø§
    if st.session_state.df is None:
        st.warning("Please upload a dataset first from the Home tab.")
        st.stop()

    if not st.session_state.get("cleaned", False):
        st.warning("Please complete the Data Cleaning step first.")
        st.stop()

    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ© Ø¯Ø§Ø¦Ù…Ù‹Ø§
    current_df = st.session_state.cleaned_df

    # ------------------ Ø§Ø®ØªÙŠØ§Ø± Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ------------------
    problem = st.selectbox(
        "Problem Type",
        ["Classification", "Regression", "Clustering"],
        key="problem_select"
    )

    # ------------------ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù‡Ø¯Ù (Target) ------------------
    if problem != "Clustering":
        target = st.selectbox(
            "Select target column",
            options=current_df.columns.tolist(),
            index=0,
            key="target_select"
        )
    else:
        target = None

    # ------------------ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª ------------------
    available_models_dict = {
        "Classification": [
            'LogisticRegression', 'RandomForestClassifier', 'XGBClassifier',
            'SVC', 'KNeighborsClassifier', 'DecisionTreeClassifier'
        ],
        "Regression": [
            'LinearRegression', 'RandomForestRegressor', 'XGBRegressor',
            'SVR', 'KNeighborsRegressor', 'DecisionTreeRegressor'
        ],
        "Clustering": ['KMeans', 'DBSCAN']
    }

    available_models = available_models_dict.get(problem, [])

    if problem == "Clustering":
        selected_models = st.selectbox(
            "Select Clustering Algorithm", available_models, key="algo_select")
    else:
        selected_models = st.multiselect(
            "Select Models to Train",
            options=available_models,
            default=available_models[:3],
            key="models_select"
        )

    # ------------------ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ------------------
    col1, col2 = st.columns(2)
    with col1:
        search_type = st.selectbox(
            "Hyperparameter Search",
            ["Random", "Optuna", "Grid"],
            index=1,  # Optuna Ø§ÙØªØ±Ø§Ø¶ÙŠ (Ø§Ù„Ø£Ø³Ø±Ø¹ ÙˆØ§Ù„Ø£ÙØ¶Ù„)
            help="Optuna is fastest & smartest"
        )
    with col2:
        n_trials = st.slider(
            "Number of Trials (Random/Optuna)", 5, 100, 30, key="n_trials")

    # ------------------ Ø²Ø± Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ------------------
    if st.button("Start Training Models", type="primary", use_container_width=True):
        if problem != "Clustering" and not selected_models:
            st.error("Please select at least one model.")
            st.stop()

        # ØªÙ†Ø¸ÙŠÙ Ø£ÙŠ Ù†ØªØ§ÙŠØ¬ Ù‚Ø¯ÙŠÙ…Ø©
        for key in ["best_models_trained", "model_evaluation_results", "selected_best_model_name"]:
            if key in st.session_state:
                del st.session_state[key]

        # Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ ÙˆØ§Ø¬Ù‡Ø© Ø¬Ù…ÙŠÙ„Ø© ÙˆÙˆØ§Ø¶Ø­Ø©
        with st.status("Training models in progress... Please wait", expanded=True) as status:
            st.write("Preparing data and splitting...")
            progress_bar = st.progress(0)

            try:
                identify_problem(
                    df=current_df,
                    problem=problem,
                    target=target,
                    selected_models=selected_models,
                    search_type=search_type,
                    n_trials=n_trials
                )

                progress_bar.progress(100)
                status.update(
                    label="All models trained successfully!",
                    state="complete",
                    expanded=False
                )
                st.success("Training completed successfully!")
                st.balloons()

            except Exception as e:
                status.update(label="Training failed!",
                              state="error", expanded=True)
                st.exception(e)
                st.stop()

        # Ø¥Ø¬Ø¨Ø§Ø± ØªØ­Ø¯ÙŠØ« Ø§Ù„ØµÙØ­Ø© Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§ÙŠØ¬
        st.rerun()

    # ==================================================================
    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§ÙŠØ¬ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Ù‡Ù†Ø§ Ø¨ÙŠØ¸Ù‡Ø± ÙƒÙ„ Ø­Ø§Ø¬Ø© Ø¨ÙˆØ¶ÙˆØ­ ÙˆÙ…Ø§ Ø¨ÙŠØ®ØªÙÙŠØ´)
    # ==================================================================
    if st.session_state.get("best_models_trained") or st.session_state.get("best_clustering_model"):

        st.markdown("---")
        st.success("Training Completed! Here's your model comparison")

        # Ø¹Ø±Ø¶ Ù†ØªØ§ÙŠØ¬ Supervised Learning
        if st.session_state.get("best_models_trained"):
            results_df = st.session_state.get("model_evaluation_results")

            if results_df is None or results_df.empty:
                st.warning(
                    "No evaluation results found. Something went wrong during training.")
            else:
                st.subheader("Model Performance Comparison")

                # Ø¹Ø±Ø¶ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
                st.dataframe(
                    results_df.style.highlight_max(axis=0, color='#d4edda'),
                    use_container_width=True
                )

                # Ø£ÙØ¶Ù„ Ù…ÙˆØ¯ÙŠÙ„ ÙØ¹Ù„ÙŠ (Ù…Ø´ priority Ø«Ø§Ø¨Øª)
                best_row = results_df.iloc[0]
                best_name = best_row['Model']

                if problem.lower() == "regression":
                    metric_value = best_row.get(
                        'RÂ² Test', best_row.get('RÂ²', 0))
                    st.metric(
                        label="Best Model",
                        value=best_name,
                        delta=f"RÂ² = {metric_value:.4f}"
                    )
                else:  # Classification
                    metric_value = best_row.get(
                        'F1', best_row.get('Accuracy', 0))
                    st.metric(
                        label="Best Model",
                        value=best_name,
                        delta=f"F1 = {metric_value:.4f}"
                    )

                # Ø²Ø± Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù…ÙˆØ¯ÙŠÙ„
                if st.button("Save Best Model & Deploy as API", type="primary", use_container_width=True):
                    st.session_state.show_save_section = True

        # Ø¹Ø±Ø¶ Ù†ØªØ§ÙŠØ¬ Clustering
        elif st.session_state.get("best_clustering_model"):
            st.success("Clustering model trained successfully!")
            model = st.session_state.best_clustering_model
            score = st.session_state.get("best_clustering_score")
            st.metric("Best Algorithm", model.__class__.__name__,
                      delta=f"Silhouette: {score:.4f}" if score else "N/A")

            if st.button("Save Clustering Model & Deploy", type="primary"):
                st.session_state.show_save_section = True

        # ------------------ Ù‚Ø³Ù… Ø§Ù„Ø­ÙØ¸ ÙˆØ§Ù„Ù†Ø´Ø± ------------------
        if st.session_state.get("show_save_section", False):
            st.markdown("### Confirm Model Saving")

            # Ø¬Ù„Ø¨ Ø£ÙØ¶Ù„ Ù…ÙˆØ¯ÙŠÙ„ ÙØ¹Ù„ÙŠ
            if st.session_state.get("best_models_trained"):
                results_df = st.session_state.model_evaluation_results
                best_name = results_df.iloc[0]['Model']
                best_model = st.session_state.best_models_trained[best_name]
            else:
                best_model = st.session_state.best_clustering_model
                best_name = best_model.__class__.__name__

            st.success(f"Selected Model: **{best_name}**")

            if st.button("Confirm Save Model Now", type="primary", use_container_width=True):
                import os
                os.makedirs("models", exist_ok=True)

                with st.spinner("Saving model and metadata..."):
                    # Ø­ÙØ¸ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
                    import pickle
                    with open("models/best_model.pkl", "wb") as f:
                        pickle.dump(best_model, f)

                    # Ø­ÙØ¸ Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§
                    with open("models/feature_names.pkl", "wb") as f:
                        pickle.dump(st.session_state.feature_names_saved, f)
                    with open("models/original_feature_names.pkl", "wb") as f:
                        pickle.dump(st.session_state.original_feature_names, f)
                    with open("models/original_dtypes.pkl", "wb") as f:
                        pickle.dump(st.session_state.original_dtypes, f)

                    if st.session_state.target_encoder_saved is not None:
                        with open("models/target_encoder.pkl", "wb") as f:
                            pickle.dump(
                                st.session_state.target_encoder_saved, f)

                st.success("Model saved successfully!")
                st.balloons()
                st.info("Go to the **Deployment** tab and click **Run API**")


# ---------------------------------
# Deployment Page
# ---------------------------------
with tab5:
    st.subheader("ğŸš€ Deploy Trained Model as REST API")
    st.markdown("Deploy your model as an API and test predictions directly here.")
    if not os.path.exists("models/best_model.pkl"):
        st.warning("âš ï¸ No trained model found. Please complete Modeling step first.")
    else:
        if st.button("ğŸš€ Run API", help="Starts the FastAPI server locally."):
            st.info("Starting FastAPI server on http://127.0.0.1:8000")
            process = subprocess.Popen([sys.executable, "-m", "uvicorn", "pipeline.api:app", "--host", "127.0.0.1", "--port", "8000", "--reload"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
                )
            st.session_state.api_process = process  # Store process in session
            time.sleep(3)
            st.success("ğŸ‰ API is LIVE!")
            st.balloons()
            st.markdown("### ğŸ“– Open API Documentation:")
            st.markdown("[Swagger UI - http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)")
            st.code("http://127.0.0.1:8000/docs", language="text")

        # Stop button for API (Added as per requirements)
        if st.session_state.api_process is not None and st.button("ğŸ›‘ Stop API", help="Stops the running FastAPI server."):
            st.session_state.api_process.kill()
            st.session_state.api_process = None
            st.success("API stopped successfully!")

        # Inference UI: Form for input features and predict via API (Added as per requirements)
        st.subheader("ğŸ” Test Model Inference")
        st.markdown("Enter feature values below and get predictions from the deployed API.")
        if os.path.exists("models/feature_names.pkl"):
            feature_names = pickle.load(open("models/feature_names.pkl", "rb"))
            inputs = {}
            for feature in feature_names:
                inputs[feature] = st.number_input(f"{feature}", help=f"Enter value for {feature}.", value=0.0)

            if st.button("Predict", help="Send inputs to the API for prediction."):
                if st.session_state.api_process is None:
                    st.error("API not running. Start the API first.")
                else:
                    try:
                        payload = {"data": [inputs]}
                        response = requests.post("http://127.0.0.1:8000/predict", json=payload)
                        if response.status_code == 200:
                            result = response.json()
                            st.success(f"Prediction: {result['predictions']}")
                        else:
                            st.error(f"Error: {response.text}")
                    except requests.exceptions.ConnectionError:
                        st.error("API not running. Start the API first.")
        else:
            st.warning("Feature names not found. Save a model first.")